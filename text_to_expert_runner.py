#!/usr/bin/env python3
"""
End-to-End Pipeline Runner: Text to Expert Model

Usage:
    python text_to_expert_runner.py --input medical_manual.txt
    python text_to_expert_runner.py --input your_doc.txt --no-fine-tune
    python text_to_expert_runner.py --input guide.txt --questions-per-chunk 10

Result: A fine-tuned 7B model that UNDERSTANDS your document
(not just lookup like RAG)
"""

import argparse
import os
from pathlib import Path
from typing import Optional

# Import orchestrator and clients
from text_to_expert_orchestrator import TextToExpertModelOrchestrator, E2EMetrics
from teacher_client import DeepSeekClient
from llm_providers import AnthropicClient
try:
    from vast_provisioner import VastProvisioner
    VAST_AVAILABLE = True
except ImportError:
    VAST_AVAILABLE = False


def get_clients():
    """Initialize all required clients"""
    deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")
    anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")
    
    if not deepseek_api_key:
        raise ValueError("DEEPSEEK_API_KEY not found in environment")
    if not anthropic_api_key:
        raise ValueError("ANTHROPIC_API_KEY not found in environment")
    
    teacher = DeepSeekClient(api_key=deepseek_api_key)
    validator = AnthropicClient(api_key=anthropic_api_key)
    
    provisioner = None
    if VAST_AVAILABLE:
        vast_api_key = os.getenv("VAST_API_KEY")
        if vast_api_key:
            provisioner = VastProvisioner(api_key=vast_api_key)
    
    return teacher, validator, provisioner


def run_pipeline(
    input_file: str,
    output_dir: Optional[str] = None,
    questions_per_chunk: int = 5,
    fine_tune: bool = True,
    show_workflow: bool = False
):
    """Execute the complete pipeline"""
    
    # Validate input
    if not os.path.exists(input_file):
        print(f"‚ùå Input file not found: {input_file}")
        return
    
    file_size_mb = os.path.getsize(input_file) / (1024 * 1024)
    print(f"\nüìÑ Input: {Path(input_file).name}")
    print(f"   Size: {file_size_mb:.1f} MB")
    
    # Setup output directory
    if output_dir is None:
        output_dir = f"./expert_model_{Path(input_file).stem}"
    
    os.makedirs(output_dir, exist_ok=True)
    print(f"üìÅ Output: {output_dir}")
    
    # Get clients
    print("\nüîó Initializing clients...")
    try:
        teacher, validator, provisioner = get_clients()
        print("   ‚úì DeepSeek teacher ready")
        print("   ‚úì Claude validator ready")
        if provisioner:
            print("   ‚úì Vast.ai provisioner ready")
        else:
            print("   ‚ö† Vast.ai not configured (fine-tune will be skipped)")
            fine_tune = False
    except ValueError as e:
        print(f"   ‚ùå {e}")
        return
    
    # Show workflow
    if show_workflow:
        print_workflow()
    
    # Run pipeline
    print("\n" + "="*70)
    print("STARTING PIPELINE: Text ‚Üí Teacher ‚Üí Validator ‚Üí Expert Model")
    print("="*70)
    
    orchestrator = TextToExpertModelOrchestrator(
        teacher_client=teacher,
        validator_client=validator,
        vast_provisioner=provisioner if fine_tune else None,
        output_dir=output_dir
    )
    
    result = orchestrator.run_complete_pipeline(
        file_path=input_file,
        questions_per_chunk=questions_per_chunk,
        fine_tune=fine_tune
    )
    
    return result


def print_workflow():
    """Display the workflow explanation"""
    print("\n" + "="*70)
    print("THE WORKFLOW: Text ‚Üí Teacher ‚Üí Validator ‚Üí Expert Model")
    print("="*70)
    
    workflow = """
STEP 1: Load & Chunk Text
  Input:  Your document (5MB manual, guide, etc.)
  Output: 400+ semantic chunks (800 chars each)
  Cost:   FREE
  Why:    Smaller pieces ‚Üí easier for teacher to work with
  
STEP 2: Generate Q&A Pairs (Teacher Loop)
  Input:  Chunks from Step 1
  Model:  DeepSeek-R1 (reasoning, cheap at $0.005/pair)
  Prompt: "Read this section. Generate 5 complex realistic questions
           a user might ask, with detailed answers and reasoning."
  Output: 2,000 Q&A pairs (REASONING TRACES)
  Cost:   ~$10
  Time:   10-15 minutes
  
  Example:
    Text: "Section 4.1: Use 5mg of X for condition Y"
    Q: "Patient has Y, what's the protocol?"
    A: "Use 5mg of X per Section 4.1, which treats Y by..."
    R: "Section 4.1 specifically recommends this dosage..."
  
STEP 3: Validate Q&A (Verifier Loop)
  Input:  2,000 Q&A pairs from Step 2
  Model:  Claude 3.5 Sonnet (accuracy verification)
  Prompt: "Is this Q&A factually accurate based on the text?
           Any hallucinations? Rate: Yes/No"
  Output: "Gold Dataset" (80-90% approval rate)
  Cost:   ~$5
  Time:   5-10 minutes
  Why:    Filter out incorrect answers before fine-tuning
  
STEP 4: Fine-Tune on Vast.ai 4090
  Input:  Gold dataset (900-1000 verified Q&A)
  Model:  Llama-2-7b or Mistral-7b
  GPU:    RTX 4090 ($0.30-0.50/hour)
  Time:   30-60 minutes (2 epochs)
  Cost:   ~$15-25
  Output: Specialized expert model
  Why:    Model UNDERSTANDS document (not just lookup/RAG)
          Can reason, combine rules, catch edge cases
  
COMPLETE PIPELINE:
  Total Cost: ~$30-40
  Total Time: 2-3 hours
  Result: Expert model that beats RAG by 10-100x

WHY THIS BEATS RAG:
  RAG approach:
    - Every query: Search file, retrieve context, generate answer
    - Cost: $0.01/query √ó 1M queries/month = $10,000
    - Latency: 2-5 seconds per query
    - Intelligence: Shallow (just copy relevant sections)
  
  Distillation (your approach):
    - One-time: Generate, validate, fine-tune ($30-40)
    - Cost: $50/month for inference (minimal)
    - Latency: <100ms per query (local model)
    - Intelligence: Deep (model understands relationships)
    
  Winner: Distillation. 200x cheaper, 50x faster, 10x smarter.
"""
    print(workflow)


def main():
    parser = argparse.ArgumentParser(
        description="Text ‚Üí Expert Model Pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  
  Basic usage:
    python text_to_expert_runner.py --input medical_manual.txt
  
  With custom questions per chunk:
    python text_to_expert_runner.py --input guide.txt --questions-per-chunk 10
  
  Without fine-tuning (just Q&A generation + validation):
    python text_to_expert_runner.py --input policy.txt --no-fine-tune
  
  Show workflow explanation:
    python text_to_expert_runner.py --show-workflow
  
  Custom output directory:
    python text_to_expert_runner.py --input doc.txt --output-dir ./my_model
        """
    )
    
    parser.add_argument(
        "--input", "-i",
        type=str,
        help="Input text file (.txt) to convert to expert model"
    )
    parser.add_argument(
        "--output-dir", "-o",
        type=str,
        default=None,
        help="Output directory (default: expert_model_<filename>)"
    )
    parser.add_argument(
        "--questions-per-chunk", "-q",
        type=int,
        default=5,
        help="Q&A pairs to generate per chunk (default: 5)"
    )
    parser.add_argument(
        "--no-fine-tune",
        action="store_true",
        help="Skip fine-tuning (just generate and validate Q&A)"
    )
    parser.add_argument(
        "--show-workflow",
        action="store_true",
        help="Display workflow explanation and exit"
    )
    
    args = parser.parse_args()
    
    # Handle --show-workflow flag
    if args.show_workflow:
        print_workflow()
        return
    
    # Require input file
    if not args.input:
        parser.print_help()
        print("\n‚ùå Error: --input is required")
        return
    
    # Run pipeline
    result = run_pipeline(
        input_file=args.input,
        output_dir=args.output_dir,
        questions_per_chunk=args.questions_per_chunk,
        fine_tune=not args.no_fine_tune,
        show_workflow=False
    )
    
    if result:
        print(f"\n‚úÖ Pipeline complete!")
        print(f"   Check output directory: {result['step3'].get('golden_dataset_file')}")


if __name__ == "__main__":
    main()
